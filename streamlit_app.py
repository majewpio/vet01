import streamlit as st
import pandas as pd
import numpy as np
import random
import plotly.express as px
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier # Bƒôdziemy symulowaƒá, ale dobrze mieƒá import

# --- Konfiguracja i dane sta≈Çe ---
DATA_FILE = 'data.csv'
PRODUCTS = {
    "Ultrasonografy przeno≈õne": ["USG-Mobile-Pro", "USG-Portable-Lite", "USG-Clinic-Max"],
    "G≈Çowice USG": ["Glowica-Liniowa-Slim", "Glowica-Konweksowa-Pro", "Glowica-Mikrokonweksowa-Vet"],
    "Platforma TU2": ["TU2-Standard", "TU2-Pro", "TU2-Enterprise"],
    "Modu≈Çy AI w TU2": ["AI-Module-Basic", "AI-Module-Advanced"],
    "Chmura danych (Cloud)": ["Cloud-50GB", "Cloud-200GB", "Cloud-Unlimited"],
    "Obs≈Çuga techniczna": ["Serwis-Standard", "Serwis-Premium"],
    "Szkolenia / edukacja": ["Webinar-Diagnostyka", "Kurs-AI-Wet", "Onboarding-Pro"],
    "Sprzƒôt towarzyszƒÖcy": ["Etui-USG", "Uchwyt-Scannerski"]
}

# Symulowane dane do logowania
VALID_USERNAME = "handlowiec"
VALID_PASSWORD = "veteye_ai"

# --- Funkcje pomocnicze ---

@st.cache_data # Cache'owanie danych, aby aplikacja dzia≈Ça≈Ça szybciej
def load_data():
    try:
        df = pd.read_csv(DATA_FILE)
        return df
    except FileNotFoundError:
        st.error(f"Plik danych '{DATA_FILE}' nie zosta≈Ç znaleziony. Upewnij siƒô, ≈ºe jest w tym samym katalogu co aplikacja.")
        st.stop()

@st.cache_resource # Cache'owanie modeli, aby nie trenowa≈Çy siƒô przy ka≈ºdej interakcji
def train_and_simulate_models(df_raw):
    # Kopia danych, aby nie modyfikowaƒá oryginalnego DataFrame
    df = df_raw.copy()

    # Przygotowanie danych (kodowanie kategorialnych)
    le = LabelEncoder()
    df['segment_encoded'] = le.fit_transform(df['segment'])
    # country celowo pomijamy w modelu zgodnie z ustaleniami
    
    features = [
        "clinic_size", "devices_owned", "last_purchase_days_ago", "purchase_count",
        "avg_purchase_value", "tu2_active", "tu2_sessions_last_30d", "ai_usage_ratio",
        "last_contact_days_ago", "open_rate", "click_rate", "support_tickets_last_6m",
        "segment_encoded"
    ]
    
    X = df[features]
    
    # Symulacja trenowania modeli (dla POC nie trenujemy realnie na 1000 rekord√≥w)
    # W prawdziwym systemie by≈Çoby to:
    # model_buy = XGBClassifier().fit(X, df['buy_label'])
    # model_churn = XGBClassifier().fit(X, df['churn_label'])
    
    # Na potrzeby POC i DEMONSTRATORA: symulujemy wyniki scoringu
    # Wytrenujmy bardzo proste modele, aby by≈Ço co≈õ realnego, ale szybko
    X_train, X_test, y_buy_train, y_buy_test, y_churn_train, y_churn_test = train_test_split(
        X, df['buy_label'], df['churn_label'], test_size=0.2, random_state=42
    )

    # Model Sprzeda≈ºowy
    model_buy = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
    model_buy.fit(X_train, y_buy_train)
    df['upsell_propensity_score'] = model_buy.predict_proba(X)[:, 1] # Prawdopodobie≈Ñstwo klasy 1

    # Model Antychurnowy
    model_churn = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
    model_churn.fit(X_train, y_churn_train)
    df['churn_risk_score'] = model_churn.predict_proba(X)[:, 1] # Prawdopodobie≈Ñstwo klasy 1

    # Dodaj symulowane SHAP values dla demo
    # W prawdziwym systemie u≈ºyliby≈õmy shap.Explainer i shap.values
    # Tutaj symulujemy, ≈ºe niekt√≥re cechy majƒÖ wiƒôkszy wp≈Çyw
    
    # Lista kluczowych cech dla SHAP (zgodnie z naszymi cechami)
    shap_features_map_buy = {
        'purchase_count': 0.2, 'avg_purchase_value': 0.15, 'tu2_active': 0.1, 
        'tu2_sessions_last_30d': 0.25, 'click_rate': 0.1, 'devices_owned': 0.2
    }
    shap_features_map_churn = {
        'tu2_sessions_last_30d': -0.3, 'last_contact_days_ago': -0.2, 'support_tickets_last_6m': -0.2,
        'tu2_active': -0.1, 'ai_usage_ratio': -0.1, 'last_purchase_days_ago': -0.1
    }

    return df, model_buy, model_churn, features, shap_features_map_buy, shap_features_map_churn

def get_product_recommendation(client_data):
    # Logika oparta na regu≈Çach eksperckich
    # Przyk≈Çad: Je≈õli klient ma tylko 1 urzƒÖdzenie i wysokƒÖ aktywno≈õƒá TU2, ale niski AI usage ratio -> zaproponuj modu≈Ç AI lub szkolenie
    # Je≈õli klient ma wiele urzƒÖdze≈Ñ i wysokƒÖ warto≈õƒá zakupu, ale dawno nie kupowa≈Ç -> zaproponuj nowe USG lub upgrade
    # Je≈õli klient ma wysoki churn_risk_score -> zaproponuj serwis premium / wsparcie
    
    if client_data['churn_risk_score'] > 0.6: # Wysokie ryzyko churnu
        return random.choice(PRODUCTS["Obs≈Çuga techniczna"] + PRODUCTS["Szkolenia / edukacja"]) + ", aby poprawiƒá do≈õwiadczenia."
    elif client_data['upsell_propensity_score'] > 0.7: # Wysoki potencja≈Ç upsell
        if client_data['devices_owned'] < 3 and client_data['tu2_active'] == 1 and client_data['ai_usage_ratio'] < 0.5:
            return random.choice(PRODUCTS["Modu≈Çy AI w TU2"]) + ", aby zwiƒôkszyƒá funkcjonalno≈õƒá."
        elif client_data['last_purchase_days_ago'] > 180 and client_data['avg_purchase_value'] > 10000:
            return random.choice(PRODUCTS["Ultrasonografy przeno≈õne"]) + ", aby od≈õwie≈ºyƒá sprzƒôt."
        elif client_data['devices_owned'] >=3 and client_data['tu2_active'] == 1 and client_data['tu2_sessions_last_30d'] > 30:
            return random.choice(PRODUCTS["G≈Çowice USG"]) + ", aby rozszerzyƒá diagnostykƒô."
        else: # Og√≥lna rekomendacja dla wysokiego potencja≈Çu
            return random.choice(PRODUCTS["Chmura danych (Cloud)"] + PRODUCTS["Platforma TU2"]) + ", aby usprawniƒá przep≈Çyw pracy."
    else: # Niska ocena, og√≥lna propozycja
        return "Brak konkretnej rekomendacji, sugerowane utrzymanie relacji."

def get_contact_strategy(client_data):
    if client_data['churn_risk_score'] > 0.6:
        return "PILNY TELEFON - dzia≈Ç Retencji / Customer Success."
    elif client_data['upsell_propensity_score'] > 0.7:
        return "TELEFON + E-MAIL z ofertƒÖ DEMO."
    elif client_data['upsell_propensity_score'] > 0.5:
        return "E-MAIL z personalizowanƒÖ ofertƒÖ."
    else:
        return "Standardowy kontakt (np. newsletter, przypomnienie)."

def get_conversation_script(client_data, recommendation):
    if "PILNY TELEFON" in get_contact_strategy(client_data):
        return f"""
        **Scenariusz rozmowy (Retencja):**
        "Dzie≈Ñ dobry, dzwoniƒô z Vet-Eye S.A. w sprawie Pa≈Ñstwa do≈õwiadcze≈Ñ z naszym sprzƒôtem/us≈Çugami. Zauwa≈ºyli≈õmy, ≈ºe Pa≈Ñstwa aktywno≈õƒá nieco spad≈Ça/mogƒÖ Pa≈Ñstwo potrzebowaƒá wsparcia w zwiƒÖzku z {recommendation}. Chcieliby≈õmy upewniƒá siƒô, ≈ºe wszystko jest w porzƒÖdku i zaoferowaƒá pomoc, aby w pe≈Çni wykorzystaƒá potencja≈Ç Vet-Eye. Czy jest co≈õ, w czym mo≈ºemy Pa≈Ñstwu pom√≥c?"
        """
    elif client_data['upsell_propensity_score'] > 0.7:
        return f"""
        **Scenariusz rozmowy (Sprzeda≈º/Upsell):**
        "Dzie≈Ñ dobry, dzwoniƒô z Vet-Eye S.A. Z naszej analizy wynika, ≈ºe Pa≈Ñstwa klinika/praktyka mo≈ºe znaczƒÖco zyskaƒá na wdro≈ºeniu {recommendation}. Wielu naszych klient√≥w w Pa≈Ñstwa segmencie z sukcesem wdro≈ºy≈Ço to rozwiƒÖzanie, poprawiajƒÖc... [wymie≈Ñ korzy≈õci]. Czy byliby Pa≈Ñstwo zainteresowani kr√≥tkƒÖ prezentacjƒÖ online, aby≈õmy mogli om√≥wiƒá szczeg√≥≈Çy?"
        """
    else:
        return "Brak specyficznego skryptu. Standardowe zapytanie o zadowolenie i potrzeby."

# --- Funkcja do symulacji SHAP ---
def plot_simulated_shap(features, client_data, shap_map, title="Wp≈Çyw cech na wynik"):
    # Tworzymy s≈Çownik z warto≈õciami SHAP dla konkretnego klienta
    # Dla POC, bierzemy sta≈Çe wagi z mapy i mno≈ºymy przez warto≈õƒá cechy (lub jej binarnƒÖ reprezentacjƒô)
    shap_values_dict = {}
    for feature, base_weight in shap_map.items():
        if feature in client_data:
            value = client_data[feature]
            # Prosta symulacja: je≈õli cecha jest aktywna/du≈ºa, jej wp≈Çyw jest silniejszy
            if 'active' in feature or 'sessions' in feature or 'ratio' in feature or 'count' in feature or 'value' in feature:
                shap_value = base_weight * (value / np.mean(client_data[feature] if isinstance(client_data, pd.DataFrame) else [client_data[f] for f in features])) # Scale by average
            elif 'contact' in feature or 'purchase_days_ago' in feature or 'tickets' in feature: # Im wiƒôksza, tym gorsza dla churn
                shap_value = base_weight * (value / np.mean(client_data[feature] if isinstance(client_data, pd.DataFrame) else [client_data[f] for f in features]))
            else: # binarna lub inna
                shap_value = base_weight * value
            shap_values_dict[feature] = shap_value
        else: # Handle cases where feature might not be directly in client_data (e.g. encoded)
             shap_values_dict[feature] = 0.0 # No impact

    # Sortowanie dla lepszej wizualizacji (najwiƒôkszy wp≈Çyw na g√≥rze)
    sorted_shap_values = sorted(shap_values_dict.items(), key=lambda item: abs(item[1]), reverse=True)
    
    # Wybieramy tylko te, kt√≥re majƒÖ niezerowy wp≈Çyw
    filtered_shap_values = [(k, v) for k, v in sorted_shap_values if abs(v) > 0.01]

    # Stworzenie DataFrame dla Plotly
    if not filtered_shap_values:
        st.write("Brak znaczƒÖcych wp≈Çyw√≥w cech do wizualizacji dla tego klienta.")
        return

    shap_df = pd.DataFrame(filtered_shap_values, columns=['Cecha', 'Wp≈Çyw SHAP (symulowany)'])
    
    fig = px.bar(shap_df, x='Wp≈Çyw SHAP (symulowany)', y='Cecha', orientation='h',
                 title=title,
                 color='Wp≈Çyw SHAP (symulowany)',
                 color_continuous_scale=px.colors.diverging.RdYlGn, # Czerwony dla negatywnego, zielony dla pozytywnego
                 labels={'Wp≈Çyw SHAP (symulowany)': 'Wp≈Çyw na wynik modelu'},
                 height=max(400, len(filtered_shap_values) * 50))
    fig.update_layout(showlegend=False, xaxis_title="Wp≈Çyw", yaxis_title="Cecha")
    st.plotly_chart(fig, use_container_width=True)


# --- G≈Ç√≥wna aplikacja Streamlit ---

st.set_page_config(
    page_title="Vet-Eye S.A. | Inteligentny CRM",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Sesja logowania
if 'logged_in' not in st.session_state:
    st.session_state['logged_in'] = False

if not st.session_state['logged_in']:
    st.title("Logowanie do Inteligentnego CRM Vet-Eye S.A.")
    with st.form("login_form"):
        username = st.text_input("Nazwa u≈ºytkownika")
        password = st.text_input("Has≈Ço", type="password")
        submit_button = st.form_submit_button("Zaloguj")

        if submit_button:
            if username == VALID_USERNAME and password == VALID_PASSWORD:
                st.session_state['logged_in'] = True
                st.success("Zalogowano pomy≈õlnie!")
                st.experimental_rerun() # Od≈õwie≈º stronƒô po zalogowaniu
            else:
                st.error("Nieprawid≈Çowa nazwa u≈ºytkownika lub has≈Ço.")
    st.stop() # Zatrzymuje wy≈õwietlanie reszty aplikacji, je≈õli nie zalogowano

# Je≈õli zalogowano, kontynuuj
st.sidebar.title("Vet-Eye S.A.")
st.sidebar.header("Inteligentny CRM")

menu_options = {
    "Dashboard": "üè† Dashboard",
    "Klienci & Scoring": "üë• Klienci & Scoring",
    "Analiza Modeli AI": "üß† Analiza Modeli AI",
    "Opis Projektu & POC": "üìù Opis Projektu & POC"
}
selected_option = st.sidebar.radio("Nawigacja", list(menu_options.keys()), format_func=lambda x: menu_options[x])

# Wyloguj
if st.sidebar.button("Wyloguj"):
    st.session_state['logged_in'] = False
    st.experimental_rerun()


# --- ≈Åadowanie i przetwarzanie danych ---
df_raw = load_data()
df, model_buy, model_churn, features_for_models, shap_map_buy, shap_map_churn = train_and_simulate_models(df_raw.copy())


# --- Sekcja: Dashboard ---
if selected_option == "Dashboard":
    st.header("Dashboard PrzeglƒÖdowy")
    st.write("Szybki wglƒÖd w kluczowe wska≈∫niki systemu Inteligentnego CRM.")

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Liczba Klient√≥w", len(df))
    with col2:
        st.metric("≈öredni Potencja≈Ç Sprzeda≈ºowy", f"{df['upsell_propensity_score'].mean():.2f}")
    with col3:
        st.metric("≈örednie Ryzyko Churnu", f"{df['churn_risk_score'].mean():.2f}")

    st.markdown("---")

    st.subheader("Top 10 Klient√≥w z Najwy≈ºszym Potencja≈Çem Sprzeda≈ºowym")
    top_upsell = df.sort_values(by='upsell_propensity_score', ascending=False).head(10)
    st.dataframe(top_upsell[['client_id', 'segment', 'country', 'upsell_propensity_score', 'churn_risk_score']].style.format({
        'upsell_propensity_score': "{:.2f}", 'churn_risk_score': "{:.2f}"
    }), hide_index=True)

    st.subheader("Top 10 Klient√≥w z Najwy≈ºszym Ryzykiem Churnu")
    top_churn = df.sort_values(by='churn_risk_score', ascending=False).head(10)
    st.dataframe(top_churn[['client_id', 'segment', 'country', 'upsell_propensity_score', 'churn_risk_score']].style.format({
        'upsell_propensity_score': "{:.2f}", 'churn_risk_score': "{:.2f}"
    }), hide_index=True)

    st.markdown("---")
    
    col_chart1, col_chart2 = st.columns(2)
    with col_chart1:
        fig_upsell_dist = px.histogram(df, x='upsell_propensity_score', nbins=20, 
                                       title='Rozk≈Çad Potencja≈Çu Sprzeda≈ºowego',
                                       labels={'upsell_propensity_score': 'Potencja≈Ç Sprzeda≈ºowy (0-1)'})
        st.plotly_chart(fig_upsell_dist, use_container_width=True)
    with col_chart2:
        fig_churn_dist = px.histogram(df, x='churn_risk_score', nbins=20,
                                      title='Rozk≈Çad Ryzyka Churnu',
                                      labels={'churn_risk_score': 'Ryzyko Churnu (0-1)'})
        st.plotly_chart(fig_churn_dist, use_container_width=True)


# --- Sekcja: Klienci & Scoring ---
elif selected_option == "Klienci & Scoring":
    st.header("Lista Klient√≥w i Ich Scoring")
    st.write("Wyszukaj klient√≥w i zobacz szczeg√≥≈Çowe informacje o ich potencjale sprzeda≈ºowym i ryzyku odej≈õcia.")

    search_query = st.text_input("Wyszukaj klienta po ID lub segmencie:", "")
    filtered_df = df[df['client_id'].str.contains(search_query, case=False) | 
                     df['segment'].str.contains(search_query, case=False)]

    st.dataframe(filtered_df[['client_id', 'segment', 'country', 'clinic_size', 'devices_owned', 
                               'upsell_propensity_score', 'churn_risk_score']].style.format({
                                   'upsell_propensity_score': "{:.2f}", 
                                   'churn_risk_score': "{:.2f}"
                               }).background_gradient(cmap='Greens', subset=['upsell_propensity_score']) # Zielony dla wysokiej sprzedazy
                               .background_gradient(cmap='Reds', subset=['churn_risk_score']), # Czerwony dla wysokiego churnu
                               hide_index=True, use_container_width=True)

    st.markdown("---")
    st.subheader("Szczeg√≥≈Çy Klienta i Rekomendacje")
    
    selected_client_id = st.selectbox(
        "Wybierz ID klienta, aby zobaczyƒá szczeg√≥≈Çy:",
        options=['Wybierz klienta'] + filtered_df['client_id'].tolist()
    )

    if selected_client_id != 'Wybierz klienta':
        client_data = df[df['client_id'] == selected_client_id].iloc[0]
        
        st.write(f"### Dane Klienta: {client_data['client_id']} ({client_data['segment']} - {client_data['country']})")
        st.write(f"**Wielko≈õƒá kliniki:** {int(client_data['clinic_size'])} | **UrzƒÖdze≈Ñ:** {int(client_data['devices_owned'])}")
        st.write(f"**Ostatni zakup:** {int(client_data['last_purchase_days_ago'])} dni temu | **Liczba zakup√≥w:** {int(client_data['purchase_count'])} | **≈örednia warto≈õƒá zakupu:** {client_data['avg_purchase_value']:.2f} USD")
        st.write(f"**TU2 Aktywne:** {'Tak' if client_data['tu2_active'] else 'Nie'} | **Sesje TU2 (ostatnie 30 dni):** {int(client_data['tu2_sessions_last_30d'])} | **Wykorzystanie AI:** {client_data['ai_usage_ratio']:.0%}")
        st.write(f"**Ostatni kontakt:** {int(client_data['last_contact_days_ago'])} dni temu | **Open Rate:** {client_data['open_rate']:.0%} | **Click Rate:** {client_data['click_rate']:.0%}")
        st.write(f"**Zg≈Çoszenia Supportowe (ostatnie 6m):** {int(client_data['support_tickets_last_6m'])}")

        st.markdown("---")

        col_scores, col_recommendations = st.columns(2)
        with col_scores:
            st.metric("Potencja≈Ç Sprzeda≈ºowy (Upsell Propensity)", f"{client_data['upsell_propensity_score']:.2f}", 
                      delta_color="off", help="Wysoka warto≈õƒá (bli≈ºej 1.0) oznacza du≈ºƒÖ szansƒô na dosprzeda≈º.")
            st.metric("Ryzyko Churnu (Churn Risk)", f"{client_data['churn_risk_score']:.2f}", 
                      delta_color="off", help="Wysoka warto≈õƒá (bli≈ºej 1.0) oznacza du≈ºe ryzyko odej≈õcia klienta.")
        
        with col_recommendations:
            recommendation = get_product_recommendation(client_data)
            contact_strategy = get_contact_strategy(client_data)
            conversation_script = get_conversation_script(client_data, recommendation)

            st.markdown(f"**Sugerowana forma kontaktu:** {contact_strategy}")
            st.markdown(f"**Rekomendowany produkt/dzia≈Çanie:** {recommendation}")
            st.markdown(f"**Sugerowany skrypt rozmowy:**")
            st.markdown(conversation_script)

        st.markdown("---")
        st.subheader("Wp≈Çyw Cech na Wynik Modelu (SHAP - Symulacja)")
        st.write("Wykres poni≈ºej pokazuje, kt√≥re cechy klienta mia≈Çy najwiƒôkszy wp≈Çyw na jego scoring.")

        # Wykres SHAP dla Potencja≈Çu Sprzeda≈ºowego
        plot_simulated_shap(features_for_models, client_data, shap_map_buy, 
                            title=f"Wp≈Çyw cech na Potencja≈Ç Sprzeda≈ºowy ({client_data['client_id']})")
        
        # Wykres SHAP dla Ryzyka Churnu
        plot_simulated_shap(features_for_models, client_data, shap_map_churn, 
                            title=f"Wp≈Çyw cech na Ryzyko Churnu ({client_data['client_id']})")

# --- Sekcja: Analiza Modeli AI ---
elif selected_option == "Analiza Modeli AI":
    st.header("Dashboard Analizy Modeli AI")
    st.write("PrzeglƒÖd og√≥lnej skuteczno≈õci i dzia≈Çania modeli predykcyjnych.")

    st.subheader("Rozk≈Çad Potencja≈Çu Sprzeda≈ºowego wg Segmentu")
    fig_upsell_segment = px.box(df, x='segment', y='upsell_propensity_score', 
                                title='Potencja≈Ç Sprzeda≈ºowy wg Segmentu Klienta',
                                color='segment',
                                labels={'upsell_propensity_score': 'Potencja≈Ç Sprzeda≈ºowy', 'segment': 'Segment'})
    st.plotly_chart(fig_upsell_segment, use_container_width=True)

    st.subheader("Rozk≈Çad Ryzyka Churnu wg Segmentu")
    fig_churn_segment = px.box(df, x='segment', y='churn_risk_score', 
                               title='Ryzyko Churnu wg Segmentu Klienta',
                               color='segment',
                               labels={'churn_risk_score': 'Ryzyko Churnu', 'segment': 'Segment'})
    st.plotly_chart(fig_churn_segment, use_container_width=True)

    st.markdown("---")
    st.subheader("Korelacja miƒôdzy aktywno≈õciƒÖ TU2 a Churnem")
    fig_scatter = px.scatter(df, x='tu2_sessions_last_30d', y='churn_risk_score', 
                             color='segment', hover_name='client_id',
                             title='Sesje TU2 vs Ryzyko Churnu',
                             labels={'tu2_sessions_last_30d': 'Sesje TU2 (ostatnie 30 dni)', 'churn_risk_score': 'Ryzyko Churnu'})
    st.plotly_chart(fig_scatter, use_container_width=True)

    st.markdown("---")
    st.subheader("Wa≈ºno≈õƒá Cech dla Modeli (Globalna)")
    st.write("Oto symulowany globalny ranking wa≈ºno≈õci cech, kt√≥re najbardziej wp≈ÇywajƒÖ na decyzje modeli (XGBoost Feature Importance).")
    
    # Symulacja Feature Importance (w prawdziwym modelu by≈Çoby model.feature_importances_)
    st.write("### Model Potencja≈Çu Sprzeda≈ºowego")
    buy_importance_df = pd.DataFrame({
        'Cecha': ['tu2_sessions_last_30d', 'purchase_count', 'devices_owned', 'avg_purchase_value', 'click_rate', 'segment_encoded', 'clinic_size'],
        'Wa≈ºno≈õƒá': [0.25, 0.20, 0.15, 0.12, 0.10, 0.09, 0.08]
    }).sort_values(by='Wa≈ºno≈õƒá', ascending=False)
    fig_buy_imp = px.bar(buy_importance_df, x='Wa≈ºno≈õƒá', y='Cecha', orientation='h',
                         title='Wa≈ºno≈õƒá Cech - Potencja≈Ç Sprzeda≈ºowy')
    st.plotly_chart(fig_buy_imp, use_container_width=True)

    st.write("### Model Ryzyka Churnu")
    churn_importance_df = pd.DataFrame({
        'Cecha': ['tu2_sessions_last_30d', 'last_contact_days_ago', 'support_tickets_last_6m', 'tu2_active', 'ai_usage_ratio', 'last_purchase_days_ago'],
        'Wa≈ºno≈õƒá': [0.30, 0.20, 0.15, 0.12, 0.10, 0.08]
    }).sort_values(by='Wa≈ºno≈õƒá', ascending=False)
    fig_churn_imp = px.bar(churn_importance_df, x='Wa≈ºno≈õƒá', y='Cecha', orientation='h',
                           title='Wa≈ºno≈õƒá Cech - Ryzyko Churnu')
    st.plotly_chart(fig_churn_imp, use_container_width=True)

# --- Sekcja: Opis Projektu & POC ---
elif selected_option == "Opis Projektu & POC":
    st.header("Opis Projektu Dyplomowego Vet-Eye S.A. & Proof of Concept (POC)")

    st.subheader("Podsumowanie Projektu")
    st.markdown("""
    Projekt **Inteligentny CRM dla Vet-Eye S.A.** to wdro≈ºenie rozwiƒÖza≈Ñ AI w celu poprawy efektywno≈õci sprzeda≈ºy, zwiƒôkszenia retencji klient√≥w (churn prediction) oraz optymalizacji dzia≈Ça≈Ñ handlowc√≥w (lead scoring). Jest to praktyczne wdro≈ºenie wiedzy zdobytej na studiach podyplomowych "Biznes.AI" w Akademii Leona Ko≈∫mi≈Ñskiego.
    """)

    st.subheader("Cel POC i Demonstratora")
    st.markdown("""
    Celem **Proof of Concept (POC)** jest weryfikacja kluczowych za≈Ço≈ºe≈Ñ technicznych i biznesowych systemu AI. **Demonstrator**, kt√≥ry Pa≈Ñstwo w≈Ça≈õnie oglƒÖdajƒÖ, to dzia≈ÇajƒÖca aplikacja Streamlit, kt√≥ra wizualizuje funkcjonalno≈õci predykcyjne (scoring sprzeda≈ºowy i antychurnowy) oraz rekomendacje dla handlowc√≥w, w oparciu o syntetyczne dane CRM.
    """)

    st.subheader("1. Proces pozyskania danych niezbƒôdnych do stworzenia systemu AI")
    st.markdown("""
    * **Specyfikacja potrzebnych danych:** System wykorzystuje dane z istniejƒÖcego CRM Vet-Eye S.A., takie jak: historia zakup√≥w, aktywno≈õƒá w platformie TU2 (sesje, u≈ºycie AI), dane kontaktowe, informacje o zg≈Çoszeniach serwisowych oraz podstawowe dane demograficzne klient√≥w (segment, wielko≈õƒá kliniki, posiadane urzƒÖdzenia).
    * **Opis danych dostƒôpnych:** Wszystkie wykorzystywane dane sƒÖ symulowane na potrzeby demonstratora i pochodzƒÖ z wewnƒôtrznego systemu CRM firmy. Nie korzystano z zewnƒôtrznych zbior√≥w danych open-source.
    * **Metodologia zbierania danych:** Dane sƒÖ zbierane przez system CRM w spos√≥b ciƒÖg≈Çy. Na potrzeby modelu sƒÖ agregowane, czyszczone i przygotowywane, aby s≈Çu≈ºyƒá jako wej≈õcie do algorytm√≥w uczenia maszynowego.
    """)
    with st.expander("Dlaczego zrezygnowali≈õmy z cechy 'country' w modelu?"):
        st.markdown("""
        W poczƒÖtkowej fazie projektu, zmienna `country` (kraj) by≈Ça brana pod uwagƒô jako potencjalny predyktor. Jednak≈ºe, z uwagi na **dominujƒÖcƒÖ wiƒôkszo≈õƒá danych pochodzƒÖcych z rynku polskiego** w dostƒôpnym zbiorze danych (co jest zgodne z za≈Ço≈ºeniem pilota≈ºu na rynku polskim), oraz **potencjalne niesp√≥jno≈õci w jako≈õci danych dla pozosta≈Çych, s≈Çabo reprezentowanych rynk√≥w**, zdecydowali≈õmy siƒô wykluczyƒá jƒÖ z bie≈ºƒÖcej wersji modeli XGBoost.
        
        Takie podej≈õcie pozwala nam skupiƒá siƒô na najsilniejszych predyktorach i zapewnia wiƒôkszƒÖ stabilno≈õƒá modelu dla g≈Ç√≥wnego rynku, jednocze≈õnie podkre≈õlajƒÖc potrzebƒô **rozbudowy i standaryzacji danych miƒôdzynarodowych w przysz≈Çych fazach projektu**, je≈õli Vet-Eye S.A. bƒôdzie rozszerzaƒá swoje dzia≈Çania AI globalnie. Jest to przyk≈Çad **pragmatycznego zarzƒÖdzania zakresem i jako≈õciƒÖ danych** w projekcie Data Science.
        """)


    st.subheader("2. Wyb√≥r i opis odpowiedniej technologii")
    st.markdown("""
    * **Hardware (in-house vs cloud):**
        * **Wyb√≥r Vet-Eye S.A.:** Firma Vet-Eye S.A. korzysta z **dzier≈ºawionych centr√≥w danych (in-house)**, co zapewnia wysokƒÖ dostƒôpno≈õƒá (99,997%) i pe≈ÇnƒÖ kontrolƒô nad danymi. Instalacja AI bƒôdzie realizowana przez firmƒô zewnƒôtrznƒÖ.
        * **Uzasadnienie:** In-house (lub dzier≈ºawa) jest preferowane ze wzglƒôdu na **bezpiecze≈Ñstwo i zgodno≈õƒá z RODO/AI Act**, kluczowƒÖ w sektorze medycznym. Zapewnia to r√≥wnie≈º ni≈ºszƒÖ latencjƒô i potencjalnie ni≈ºsze koszty w d≈Çugoterminowej perspektywie przy du≈ºej skali danych i ciƒÖg≈Çym u≈ºyciu.
        * **Dla POC:** Demonstrator dzia≈Ça na platformie Streamlit Community Cloud, symulujƒÖc dostƒôp do danych.
    * **Metody (frameworki, biblioteki):**
        * **Jƒôzyk:** Python (standard w AI/ML).
        * **Aplikacja webowa/Demonstrator:** **Streamlit** (szybkie prototypowanie interaktywnych aplikacji).
        * **Biblioteki AI/ML:** **XGBoost** (algorytm predykcyjny), **Pandas** (przetwarzanie danych), **NumPy** (operacje numeryczne), **Scikit-learn** (narzƒôdzia ML).
        * **Wizualizacja:** **Plotly** (interaktywne wykresy).
    * **Architektura systemu (blokowy schemat):**
        1.  **Modu≈Ç Pozyskiwania Danych (CRM Data Extraction):** Symulacja pobierania danych z CRM.
        2.  **Modu≈Ç Przetwarzania Danych (Data Preprocessing):** Czyszczenie, kodowanie, in≈ºynieria cech.
            * *Biblioteki:* Pandas, Scikit-learn (LabelEncoder).
        3.  **Model AI - Scoring Sprzeda≈ºowy:** Wytrenowany model klasyfikujƒÖcy.
            * *Biblioteki/Metody:* **XGBoost (XGBClassifier)**.
        4.  **Model AI - Scoring Antychurnowy:** Wytrenowany model klasyfikujƒÖcy.
            * *Biblioteki/Metody:* **XGBoost (XGBClassifier)**.
        5.  **Modu≈Ç Logiki Eksperckiej (Rule-Based Recommendations):** Generowanie rekomendacji produkt√≥w i skrypt√≥w rozm√≥w.
            * *Technologia:* Python (warunki logiczne).
        6.  **Interfejs U≈ºytkownika (Handlowca):** Wizualizacja scoring√≥w, rekomendacji, dashboard√≥w.
            * *Technologia:* **Streamlit**, Plotly.
        7.  **Baza Danych (POC):** Plik `data.csv` (symulacja danych CRM).
    """)
    with st.expander("Dlaczego dwa niezale≈ºne modele AI, a nie jeden?"):
        st.markdown("""
        Decyzja o zastosowaniu **dw√≥ch niezale≈ºnych modeli XGBoost** (dla sprzeda≈ºy i antychurnu) zamiast jednego, bardziej z≈Ço≈ºonego modelu, jest **ca≈Çkowicie zasadna** dla Vet-Eye S.A., zw≈Çaszcza na etapie POC i wczesnego wdro≈ºenia, poniewa≈º:
        * **R√≥≈ºne Cele Biznesowe:** Ka≈ºdy model adresuje odrƒôbny cel ‚Äì **zwiƒôkszenie przychod√≥w** (sprzeda≈º) vs. **zmniejszenie strat** (retencja). Mieszanie tych cel√≥w w jednym modelu mog≈Çoby obni≈ºyƒá precyzjƒô.
        * **R√≥≈ºne Zmienne Celu:** Modele przewidujƒÖ dwie odrƒôbne zmienne (`buy_label` i `churn_label`). Standardowe algorytmy klasyfikacji nie obs≈ÇugujƒÖ tego efektywnie w ramach jednego modelu.
        * **Jasna Interpretacja dla Handlowc√≥w:** Handlowcy otrzymujƒÖ **dwie jasne, niezale≈ºne informacje**: "Ten klient ma potencja≈Ç do zakupu" i "Ten klient jest zagro≈ºony odej≈õciem". To daje im pe≈Çniejszy i mniej dwuznaczny obraz sytuacji.
        * **Prostsza Architektura i Utrzymanie:** Dwa mniejsze modele sƒÖ ≈Çatwiejsze do zaimplementowania i niezale≈ºnego rozwoju. Pozwala to na szybsze wprowadzanie zmian i rozwiƒÖzywanie problem√≥w w przysz≈Ço≈õci, bez wp≈Çywu na drugi model.
        * **R√≥≈ºne Zestawy Cech Kluczowych:** Mimo ≈ºe dane wej≈õciowe sƒÖ wsp√≥lne, waga poszczeg√≥lnych cech dla predykcji sprzeda≈ºy i churnu mo≈ºe siƒô r√≥≈ºniƒá. Dwa modele pozwalajƒÖ na optymalizacjƒô pod kƒÖtem tych r√≥≈ºnic.
        """)

    st.subheader("3. Definicja i opis fazy dostosowania (trenowania) i wykorzystania systemu AI")
    st.markdown("""
    * **Proces trenowania:**
        * **Zbieranie danych treningowych:** Agregacja historycznych danych CRM (na potrzeby POC - symulacja z `data.csv`).
        * **Przygotowanie danych:** Czyszczenie, kodowanie zmiennych kategorialnych (np. `segment`).
        * **Podzia≈Ç danych:** Na zbiory treningowe i testowe w celu walidacji.
        * **Trening modeli XGBoost:** Dwa niezale≈ºne klasyfikatory XGBoost sƒÖ trenowane. Algorytm wybrany jest ze wzglƒôdu na wysokƒÖ skuteczno≈õƒá i odporno≈õƒá na przeuczenie.
        * **Walidacja:** Ocena modeli na zbiorze testowym.
    * **Szacunkowy czas treningu i koszt:**
        * **Dla POC:** Czas treningu na 1000 rekord√≥w jest **pomijalny** (sekundy). Brak koszt√≥w chmurowych, aplikacja wykorzystuje minimalne zasoby Streamlit Cloud.
        * **Dla realnego wdro≈ºenia:** Czas treningu mo≈ºe wynosiƒá od **kilku minut do kilku godzin** na dedykowanych serwerach Vet-Eye S.A., w zale≈ºno≈õci od wolumenu danych (miliony rekord√≥w) i z≈Ço≈ºono≈õci cech. Koszty zwiƒÖzane z amortyzacjƒÖ i utrzymaniem infrastruktury in-house, a nie z op≈Çatami za publicznƒÖ chmurƒô.
    """)

    st.subheader("4. Ocena efektywno≈õci systemu AI")
    st.markdown("""
    * **Techniczna:**
        * **Metryki:** Dok≈Çadno≈õƒá (Accuracy), Precyzja (Precision), Czu≈Ço≈õƒá (Recall), F1-score, AUC ROC ‚Äì stosowane do oceny poprawno≈õci predykcji.
        * **Szybko≈õƒá inferencji:** Na potrzeby POC inferencja dla pojedynczego klienta jest **natychmiastowa** (milisekundy). W realnym systemie, przetwarzanie ca≈Çej bazy jest regularne i szybkie.
        * **Funkcjonalna:** Intuicyjno≈õƒá interfejsu Streamlit, ≈Çatwo≈õƒá dostƒôpu do kluczowych informacji i rekomendacji dla handlowc√≥w.
        * **Niefunkcjonalna:** Skalowalno≈õƒá (zdolno≈õƒá do obs≈Çugi rosnƒÖcej liczby klient√≥w), niezawodno≈õƒá (wysoka dostƒôpno≈õƒá dziƒôki dw√≥m centrom danych Vet-Eye S.A.), bezpiecze≈Ñstwo (zgodno≈õƒá z RODO i AI Act).
    * **Biznesowa (ROI/Break-even):**
        * **Wzrost przychod√≥w:** Poprawa wsp√≥≈Çczynnika konwersji lead√≥w (dziƒôki lepszemu lead scoringowi) i zwiƒôkszenie ≈õredniej warto≈õci transakcji (dziƒôki rekomendacjom upsellowym).
        * **Zmniejszenie koszt√≥w:** Redukcja odp≈Çywu klient√≥w (dziƒôki predykcji churnu i proaktywnym dzia≈Çaniom retencyjnym), optymalizacja czasu pracy handlowc√≥w.
        * **Break-even Point:** Punkt, w kt√≥rym skumulowane korzy≈õci (zwiƒôkszone przychody + zmniejszone koszty) zr√≥wnajƒÖ siƒô z kosztami wdro≈ºenia i utrzymania systemu AI. **Na etapie POC sƒÖ to szacunki i prognozy**, bazujƒÖce na hipotetycznych wzrostach wska≈∫nik√≥w biznesowych.
    """)